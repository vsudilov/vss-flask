{% extends "base.html" %}
{% block content %}
<style>
  .summary-divider {
      margin: 80px 0;
  }
  .summary {
      overflow: hidden;
  }
  .summary-image.pull-left {
      margin-right: 40px;
  }
  .summary-image.pull-right {
      margin-left: 40px;
  }
  .summary-heading {
      font-size: 50px;
  }

  .footnotes {
    font-size:0.65em;
  }

  .row {
    margin-top: 10px;
    margin-bottom:10px;

  }

</style>


<div class="container">
    <div class="summary">
        <h2 class="summary-heading">
            Future of the ADS backend <br><span class="text-muted">Solr, clouds, and distributed search</span>
        </h2>

        <div class="lead">
          <p>
            As of mid 2014, the <a href="labs.adsabs.harvard.edu">ADS</a> maintains 10.4M documents, including all their data (i.e., the text) and metadata. The corpus grows at a rate of 10<sup>3</sup> to 10<sup>4</sup> documents per month. In ADS2.0, we indirectly expose this unique and rich dataset to our users via a <a href='https://github.com/romanchyla/montysolr'>custom build</a> of <a href='http://lucene.apache.org/solr/'>Solr</a>. In our case, Solr serves the dual purpose of the search index and document store<sup>1</sup>
          </p>

          <p>
            We need top performance from Solr. Our index size is already on the limits of what a single high-end server can handle under our traffic load. That's OK though, since we can distribute our index across many servers by leveraging the <a href="https://cwiki.apache.org/confluence/display/solr/SolrCloud">distributed search</a> capabilities of Solr
          </p>

          <p>
            We've elected to set up our SolrCloud using Amazon Web Services (AWS). On-demand instances allow us to  swap between different configurations of hardware, sharding, and replication. AWS enables us to find the optimal hosting infrastructure with minimal monetary commitment.
          </p>

          <p>
            The end goal is to abstract the deployment and hosting of our index. When that goal is met, handling an order of magnitude increase in traffic and/or corpus size (ADS for biologists, perhaps?) is a only<sup>2</sup> matter of making a few calls to the AWS API.
          </p>

          <div class="footnotes">
            <p id="fn1">
                1. We use a local durable data store to populate Solr, but users don't access this resource.
            </p>
            <p id="fn2">
                2. Well, and increasing our operating budget
            </p>
          </div>
         
          <!-- <pre class="prettyprint ">@task(1)</pre> -->
        </div>
    </div>
    <hr class="summary-divider" />
    <div class="summary">
        <h2 class="summary-heading">
            Benchmarking ADS <br>
            <!-- <span class="text-muted">Measuring the average user's search experience</span> -->
        </h2>
        <div class="lead">
          <h3 class="text-muted">Creating sample queries</h3>
          <p>
            <img class="summary-image img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/elasticsearch_queries.png" style="height:35%; width:35%;">
            To model an average user search, we use a set of 10,000 sequential queries parsed from the Solr logfiles starting on Jan. 1, 2014. The queries were generated by normal user traffic, and thus should be an accurate representation of actual ADS usage.

            The number of occurrences of each query is shown on the right. There are just over 7000 unique queries in the entire set. Less than 1% of the queries occur more than twice. The unusual query that occurs ~100 times is an unfielded search for "star": <code>/solr/select?wt=json&q=star</code>. We remove this query from the sample, as this query is our internal test query.
          </p>
        </div>
      </div>


      <div class="summary">
        <div class="lead">
          <h3 class="text-muted">Setting up the swarm</h3>
          <p>
            We use <a href="locust.io">locust.io</a> to create a pool of workers that queries our solr endpoint. A worker loads our sample of queries into memory, and assigns itself a unique id (UUID). The worker sends a GET request to a single <code>random.choice(urls)</code>, which cooresponds to a query from our sample. The worker blocks until it recieves a response, after which it writes transactional metadata to a logfile, including its own UUID. This process repeats indefinitely. The swarm is composed of 1-100 workers, each performing and recording their requests.
          </p>
        </div>
      </div>

      <div class="summary">
        <div class="lead">
          <h3 class="text-muted">Qtime results</h3>
          <div class="row">
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/example_benchmarking2.png">
            </div>
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-right" alt="" src="/static/images/solr_benchmarking/example_benchmarking.png">
            </div>
          </div>
          <p>
            Above are results with N=5 and N=50 workers swarming against a single server with 145GBs RAM and 24 logical cores (Xeon X5680, 2010 CPU). The top panel in each figure is the cumulative distribution of Qtimes, and the bottom panel is the raw data. The 95% Qtime is the longest time at which 95% of queries returned -- 95% of all queries took 1347ms or less when 5 workers were loading the server, and 3592ms with 50 workers. So, even though the raw data shows pronounced spikes, the cumulative distribution tells us that these long queries only occur a very small fraction of the time.
          </p>
          <p>
            Even though these long query spikes are infrequent, it could still be useful to understand what causes them. Our initial thought was that these long delays arise from garbage collection pauses: The JVM may be using all its allocated memory before removing old objects to free up memory. An analogy is cleaning the house only once it becomes completely messy as opposed to cleaning a little bit every day. If the house is big (large heap size) and full of clutter (large fraction of the heap allocated), it will take a long time to clean (long GC time).
          </p>
          <div class="row">
            <div class="col-sm-12">
              <img style="display:block; margin-left: auto; margin-right:auto; height:50%; width:50%" class="img-square img-responsive" alt="" src="/static/images/solr_benchmarking/power_spectrum_adswhy_3.png">
            </div>
          </div>
          <p>
            If these spikes in Qtime are due to GC pauses, one would expect periodicity in those big spikes. However, we don't see this: the power spectrum from the Fourier transform of the data don't show any dominant frequencies. We conclude that we aren't seeing long GC pauses, but rather just some particularly expensive queries. We have the data to coorelate query with Qtime, so this hypothesis can and eventually should be tested.
          </p>
        </div>
      </div>


    <div class="summary">
        <div class="lead">
          <h3 class="text-muted">What about distributed search?</h3>
          <p>
            <!-- <table class="table table-hover table-condensed">
              <tr>
                <td>Environment</td>
                <td>RAM</td>
                <td>CPUs</td>
              </tr>
            </table>
 -->
          </p>

          <div class="row">
            <div class="col-sm-4">
              <img class="img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/q95_vs_nworkers.png">
            </div>
            <div class="col-sm-4">
              <img class="img-square img-responsive" alt="" src="/static/images/solr_benchmarking/req_per_sec_vs_nworkers.png">
            </div>
            <div class="col-sm-4">
              <img class="img-square img-responsive pull-right" alt="" src="/static/images/solr_benchmarking/q95_vs_req_per_sec.png">
            </div>
          </div>
          <p>
            In all cases, the sharded environment outperforms the in-house setup. Qtimes are consistently and significantly lower, even for low load. In the middle figure, we can see that the in-house setup has trouble serving more than ~20 workers, while the SolrCloud setups starts to run into these limits at a higher traffic load.
          </p>

          <p>
            Though all of this data is already goes a long way in quantifying performance of a SolrCloud-based ADS backend, there are still many routes that would be interesting to explore:
              <ul>
                <li> Compare different sharding and replication factors
                <li> Check at what points the bottleneck are CPU, or I/O.
                <li> Compare different VMs hosting Solr and Zookeeper
                <li> Use a <a href="https://github.com/romanchyla/solrjmeter/tree/master/queries/adsabs">set of queries</a> that target certain Solr operations 
                <li> Measure performance under indexing and/or recovery operations
              </ul>
          </p>
        </div>
      </div>

    <!--       <hr class="summary-divider" />
          <p>
            <div class="col-sm-3">
            <img class="summary-image img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/example_benchmarking.png" style="height:45%;width:45%"><br>
            

            <img class="summary-image img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/example_benchmarking2.png" style="height:45%;width:45%">

          </p>

          <p>
            <img class="summary-image img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/power_spectrum_adswhy_3.png" style="height:50%;width:50%">
          </p>
        </div>
    </div> -->


    <hr class="summary-divider" />
    <div class="summary">
      <!--   <img class="summary-image img-suqare img-responsive pull-right" alt="Bootstrap template" src="http://placehold.it/500x500"> -->
        <a name="problems"></a><h2 class="summary-heading">
            Problems with distributed search</br><span class="text-muted">Relational data</span>
        </h2>
        <p>
          ADS2.0 supports "second-order operators" which return relational data for a set of documents. A user can give ADS2.0 the query <code>citations(chandra)</code>. In this example, Solr first finds a set of documents (~50K documents) that match the non-fielded <code>chandra</code> search. Before relevancy ranking and pagination, Solr then applies the <code>citations()</code> operator on the set, which compiles a new list of results that are populated by each document's multi-valued <code>citations</code> field (270k documents). It is this final list that gets ranked and paginated, and finally sent to the user.
        </p>

        <p>
          In a sharded environment, this process happens in each shard. The relational link between a document in shard A that references a document in shard B will not be followed. Instead, we would require that Solr export all documents at every step to some master process. The master process would then be responsible for re-sending the seeded query to both shards, repeating the process until the final query has been satisfied. This process becomes prohibitively time consuming when the initial query returns more than a few thousand documents.
        </p>

        <p>
          There are several solutions to this problem<sup>1</sup>, but none that are ideal. We test if an AWS setup of SolrCloud<sup>2</sup> with nshards=1 if able to meet our current needs.
        </p>


        <div class="footnotes">
          <p id="fn1">
              1. Sharding/routing a document based on a its relationships, or using another service optimized for performing relational lookups
          </p>
          <p id="fn1">
              2. We prefer SolrCloud style replication instead of traditional M-S replication since leader election is automated. Contrastingly, M-S replication requires one to expliticly define a leader.
          </p>
        </div>

        <div class="summary">
        <div class="lead">
          <h3 class="text-muted">Xmx results with nshards=1</h3>
          <div class="row">
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/r3.4xlarge_req_per_sec_vs_nworkers.png">
            </div>
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-right" alt="" src="/static/images/solr_benchmarking/r3.4xlarge_q95_vs_nworkers.png">
            </div>
          </div>
          <div class="row">
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-right" alt="" src="/static/images/solr_benchmarking/r3.4xlarge_q95_vs_req_per_sec.png">
            </div>
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-right" alt="" src="/static/images/solr_benchmarking/r3.4xlarge_median_qt_vs_req_per_sec.png">
            </div>
          </div>
          <p>
            We test r3.4xlarge (122 GB ram, 52 ECU) with nshards=1 and replication=1 for different Xmx of 75%,60%,45%,35% of the physical ram (92214, 73771, 55328, and 43033 MB). Below are the same figures with r3.xlarge nshards=2 setup.
          </p>

          <div class="row">
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/r3.4xlarge_req_per_sec_vs_nworkers_all.png">
            </div>
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-right" alt="" src="/static/images/solr_benchmarking/r3.4xlarge_q95_vs_nworkers_all.png">
            </div>
          </div>
          <div class="row">
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-right" alt="" src="/static/images/solr_benchmarking/r3.4xlarge_q95_vs_req_per_sec_all.png">
            </div>
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-right" alt="" src="/static/images/solr_benchmarking/r3.4xlarge_median_qt_vs_req_per_sec_all.png">
            </div>
          </div>          
      </div>

    </div>
    <hr />
{% endblock %}