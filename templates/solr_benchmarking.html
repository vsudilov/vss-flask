{% extends "base.html" %}
{% block content %}
<style>
  .summary-divider {
      margin: 80px 0;
  }
  .summary {
      overflow: hidden;
  }
  .summary-image.pull-left {
      margin-right: 40px;
  }
  .summary-image.pull-right {
      margin-left: 40px;
  }
  .summary-heading {
      font-size: 50px;
  }

  .footnotes {
    font-size:0.65em;
  }

  .row {
    margin-top: 10px;
    margin-bottom:10px;

  }

</style>


<div class="container">
    <div class="summary">
        <h2 class="summary-heading">
            Future of the ADS backend <br><span class="text-muted">Solr, clouds, and distributed search</span>
        </h2>

        <div class="lead">
          <p>
            As of mid 2014, the ADS maintains 10.4M documents, including all data (i.e., the text) and metadata related to a publication. The corpus grows at a rate of 10<sup>3</sup> to 10<sup>4</sup> documents per month. In ADS2.0, we indirectly expose this dataset to our users via a <a href='https://github.com/romanchyla/montysolr'>custom build</a> of <a href='http://lucene.apache.org/solr/'>Solr</a>, which in our case serves the dual purpose of the search index and document store<sup><a href="#fn1" id="ref1">1</a></sup>
          </p>

          <p>
            Naturally, we require top performance from Solr. Our index size is already on the limits of what a single high-end server can handle under our traffic load. That's OK though, since we can distribute our index across many servers by leveraging the built-in <a href="https://cwiki.apache.org/confluence/display/solr/SolrCloud">distributed search</a> capabilities of Solr
          </p>

          <p>
            We've elected to set up our SolrCloud using Amazon Web Services. On-demand instances allow us to quickly swap between different configurations of hardware, sharding, and replication in AWS quickly and efficiently. This enables us to optimize these parameters minimal monetary commitment, and in the future change the configuration based on increased traffic and/or index size. 
          </p>

          <p>
            The end goal is to abstract the deployment and hosting of our index to such an extent that handling an order of magnitude increase in traffic and/or corpus size (ADS for biologists, perhaps?) is a only<sup><a href="#fn2" id="ref1">2</a></sup> matter of making a few calls to the AWS API.
          </p>

          <div class="footnotes">
            <p id="fn1">
                1. We use a local durable data store from which we populate Solr, but neither Solr nor end users access this resource.<a href="#ref1" title="Jump back to footnote 1 in the text.">↩</a>
            </p>
            <p id="fn2">
                2. Well, and finding a way to increase our operating budget<a href="#ref1" title="Jump back to footnote 1 in the text.">↩</a>
            </p>
          </div>
         
          <!-- <pre class="prettyprint ">@task(1)</pre> -->
        </div>
    </div>
    <hr class="summary-divider" />
    <div class="summary">
        <h2 class="summary-heading">
            Benchmarking ADS <br>
            <!-- <span class="text-muted">Measuring the average user's search experience</span> -->
        </h2>
        <div class="lead">
          <h3 class="text-muted">Creating sample queries</h3>
          <p>
            <img class="summary-image img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/elasticsearch_queries.png" style="height:35%; width:35%;">
            To model an average user search, we use a set of 10,000 sequential queries parsed from the Solr logfiles starting on Jan. 1, 2014. The queries were generated by normal user traffic, and thus are expected be an accurate representation of actual ADS usage.

            The number of occurances of each query is shown on the right. There are just over 7000 unique queries in the entire set. Less than 1% of the queries occur more than twice. The unusual query that occurs ~100 times happens to be an unfielded search for "star": <code>/solr/select?wt=json&q=star</code>. We remove this query from the sample.
          </p>
        </div>
      </div>


      <div class="summary">
        <div class="lead">
          <h3 class="text-muted">Setting up the swarm</h3>
          <p>
            We use <a href="locust.io">locust.io</a> to create a pool of workers that queries our solr endpoint. Our sample of queries is loaded into memory on worker import, and each worker is assigned a UUID. A worker sends a GET request to a single <code>random.choice(urls)</code>, which cooresponds to a query from our sample. The worker blocks until it recieves a response, after which it writes transactional metadata to a logfile, including its own UUID. This process repeats indefinitely. The swarm is composed of 1-100 of these workers, each simultaneously performing and recording their requests. 
          </p>
        </div>
      </div>

      <div class="summary">
        <div class="lead">
          <h3 class="text-muted">Qtime results</h3>
          <div class="row">
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/example_benchmarking2.png">
            </div>
            <div class="col-sm-6">
              <img class="img-square img-responsive pull-right" alt="" src="/static/images/solr_benchmarking/example_benchmarking.png">
            </div>
          </div>
          <p>
            Above are results from swarms with N=5 and N=50 workers against a single server with 145GBs RAM and 24 logical cores (Xeon X5680, 2010 CPU). The top panel in each figure is the cumulative distribution of Qtimes, and the bottom panel is the raw data. The 95% Qtime is the maximum time at which 95% of queries returned -- 95% of all queries took 1347ms or less when 5 workers were loading the server, and 3592ms with 50 workers. So, even though the raw data shows some really pronounced spikes, the cumulative distribution tells us that these long queries only occur a very small fraction of the time.
          </p>
          <p>
            Even though these long query spikes are infrequent, it could still be useful to understand what causes them. Our initial thought was that these long delays arise from garbage collection pauses: The JVM may be using all of its allocated memory before removing old objects to free up memory. An analogy is cleaning the house only once it becomes really messy as opposed to cleaning a little bit every day. If the house is really big (large heap size) and full of clutter (large fraction of the heap allocated), it will take a long time to clean (long GC time).
          </p>
          <div class="row">
            <div class="col-sm-12">
              <img style="display:block; margin-left: auto; margin-right:auto; height:50%; width:50%" class="img-square img-responsive" alt="" src="/static/images/solr_benchmarking/power_spectrum_adswhy_3.png">
            </div>
          </div>
          <p>
            If these queries are blocked due to GC pauses, and if on average there are the same number an complexity of queries coming in, we should expect some periodicity in those big spikes. But, the power spectrum from the Fourier transform of the data don't show any dominant frequencies. We conclude that we aren't seeing long GC pauses, but rather just some queries from the sample are extremely expensive to perform. We have the data to coorelate query with average Qtime, and so this idea should be tested when we look into it.
          </p>
        </div>
      </div>


    <div class="summary">
        <div class="lead">
          <h3 class="text-muted">What about distributed search?</h3>
          <p>
            We compare the performance of our server above with a sharded environment hosted on AWS. The AWS components consist of 4 identical VMs (m3.2xlarge: 30GB ram, 26 ECU, and SSD backed EBS) that create a SolrCloud with 2 shards and a replication factor 2. The zookeeper ensemble consists of 3 VMs (m1.large: 7.5GB ram, 4 ECUs, magnetic instance store backed). The JVM of each solr instance is set to 60% of the physical ram, which is 18 GB for the m3.2xlarge instances.
          </p>

          <div class="row">
            <div class="col-sm-4">
              <img class="img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/q95_vs_nworkers.png">
            </div>
            <div class="col-sm-4">
              <img class="img-square img-responsive" alt="" src="/static/images/solr_benchmarking/req_per_sec_vs_nworkers.png">
            </div>
            <div class="col-sm-4">
              <img class="img-square img-responsive pull-right" alt="" src="/static/images/solr_benchmarking/q95_vs_req_per_sec.png">
            </div>
          </div>
          <p>
            In each figure, the blue points and red points coorespond to the in-house single shard and AWS hosted SolrCloud setups, respectively. In all cases, the sharded environment outperforms the in-house setup, at least for the hardware described above. Qtimes are consistently and significantly lower even for low load. In the middle figure, we can see that the in-house setup starts to have trouble serving more than ~20 workers, while the SolrCloud setup starts to flatten out at around 60. The scatter from the SolrCloud setup is thought to be due to a combination of using a cold EBS volumes, and the ELB in front of the setup still scaling up. 
          </p>

          <p>
            Though all of this data is already goes a long way in quantifying performance of the ADS backend, there are still many routes that would be interesting to explore:
              <ul>
                <li> Compare different sharding and replication factors
                <li> Compare different VMs hosting Solr and Zookeeper
                <li> Use a <a href="https://github.com/romanchyla/solrjmeter/tree/master/queries/adsabs">set of queries</a> that target certain Solr operations 
                <li> Measure performance under indexing and/or recovery operations
              </ul>
          </p>
        </div>
      </div>

    <!--       <hr class="summary-divider" />
          <p>
            <div class="col-sm-3">
            <img class="summary-image img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/example_benchmarking.png" style="height:45%;width:45%"><br>
            

            <img class="summary-image img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/example_benchmarking2.png" style="height:45%;width:45%">

          </p>

          <p>
            <img class="summary-image img-square img-responsive pull-left" alt="" src="/static/images/solr_benchmarking/power_spectrum_adswhy_3.png" style="height:50%;width:50%">
          </p>
        </div>
    </div> -->



<!--     <hr class="summary-divider" />
    <div class="summary">
        <img class="summary-image img-suqare img-responsive pull-right" alt="Bootstrap template" src="http://placehold.it/500x500">
        <h2 class="summary-heading">
            foo <span class="text-muted">bar</span>
        </h2>
        <p class="lead">
            baz
        </p> -->
    </div>
    <hr />
{% endblock %}